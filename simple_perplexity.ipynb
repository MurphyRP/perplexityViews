{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2bca06cc",
   "metadata": {},
   "source": [
    "# A Simple View of Perpexity..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df614495",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "Quick OpenAI connection to submit prompts and return logprobs for our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56d293b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment ready!\n",
      "OpenAI version: 1.91.0\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"viridis\")\n",
    "\n",
    "print(\"Environment ready!\")\n",
    "print(f\"OpenAI version: {openai.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922461f5",
   "metadata": {},
   "source": [
    "## API Connection and Test Function\n",
    "Just a quick sanity check so we know our keys work and the API is connecting as expected (easier to troubleshoot here than later!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e236d392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with: 'The capital of Iceland is'\n"
     ]
    }
   ],
   "source": [
    "# Load API key from config\n",
    "with open('config.json', 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "client = openai.OpenAI(\n",
    "    api_key=config['openai_api_key']\n",
    ")\n",
    "\n",
    "# Test function using Chat Completions with logprobs\n",
    "def get_completion_with_logprobs(prompt, temperature=0.7, max_tokens=10):\n",
    "    \"\"\"Get completion with token probabilities using Chat Completions API\"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4.1\",  # Latest and best model\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=temperature,\n",
    "        logprobs=True,\n",
    "        top_logprobs=5  # top 5 token probabilities\n",
    "    )\n",
    "    return response\n",
    "\n",
    "# Test with our simple example\n",
    "test_prompt = \"The capital of Iceland is\"\n",
    "print(f\"Testing with: '{test_prompt}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8990f728",
   "metadata": {},
   "source": [
    "## Token Data Extraction and Perplexity Calculation\n",
    "OpenAI is one of the few APIs that provides probabilities at the token level.  This is a click through cell but important to what we are looking for and at."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dcdd76fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_token_data(response):\n",
    "    \"\"\"Extract tokens and their probabilities from the response\"\"\"\n",
    "    tokens = []\n",
    "    probabilities = []\n",
    "    \n",
    "    if response.choices[0].logprobs and response.choices[0].logprobs.content:\n",
    "        for token_data in response.choices[0].logprobs.content:\n",
    "            tokens.append(token_data.token)\n",
    "            prob = np.exp(token_data.logprob)\n",
    "            probabilities.append(prob)\n",
    "    \n",
    "    return tokens, probabilities\n",
    "\n",
    "def calculate_perplexity(probabilities):\n",
    "    \"\"\"Calculate perplexity from token probabilities\"\"\"\n",
    "    if not probabilities:\n",
    "        return float('inf')\n",
    "    \n",
    "    # Handle zero probabilities by adding small epsilon\n",
    "    epsilon = 1e-10\n",
    "    safe_probs = [max(p, epsilon) for p in probabilities]\n",
    "    \n",
    "    # Perplexity = exp(-1/N * Î£ log(p_i))\n",
    "    log_probs = [np.log(p) for p in safe_probs]\n",
    "    avg_log_prob = np.mean(log_probs)\n",
    "    perplexity = np.exp(-avg_log_prob)\n",
    "    \n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51452734",
   "metadata": {},
   "source": [
    "## Temperature Comparison Functions\n",
    "Simple set of methods to call the OpenAI api and retrieve the response and probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "204c9a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def compare_temps(prompt, temps=[0.1, 0.7, 1.5], max_tokens=20):\n",
    "    \"\"\"Compare perplexity of same prompt at different temperatures\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for temp in temps:\n",
    "        response = get_completion_with_logprobs(prompt, temperature=temp, max_tokens=max_tokens)\n",
    "        tokens, probs = extract_token_data(response)\n",
    "        perplexity = calculate_perplexity(probs)\n",
    "        generated_text = ''.join(tokens)\n",
    "        \n",
    "        results.append({\n",
    "            'Temperature': temp,\n",
    "            'Perplexity': round(perplexity, 3),\n",
    "            'Generated Text': generated_text[:50] + ('...' if len(generated_text) > 50 else ''),\n",
    "            'Avg Confidence': round(np.mean(probs) if probs else 0, 3)\n",
    "        })\n",
    "    \n",
    "    # Return clean DataFrame instead of print + raw dict\n",
    "    df = pd.DataFrame(results)\n",
    "    return df\n",
    "\n",
    "def compare_prompts(prompts_list, temp=0.7, max_tokens=20):\n",
    "    \"\"\"Compare perplexity of different prompts at same temperature\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for prompt in prompts_list:\n",
    "        response = get_completion_with_logprobs(prompt, temperature=temp, max_tokens=max_tokens)\n",
    "        tokens, probs = extract_token_data(response)\n",
    "        perplexity = calculate_perplexity(probs)\n",
    "        generated_text = ''.join(tokens)\n",
    "        \n",
    "        results.append({\n",
    "            'Prompt': prompt,\n",
    "            'Perplexity': f\"{perplexity:.2f}\" if perplexity < 1000 else f\"{perplexity:.0f}\",\n",
    "            'Generated': generated_text[:120] + ('...' if len(generated_text) > 120 else ''),\n",
    "            'Confidence': f\"{np.mean(probs):.3f}\" if probs else \"0.000\"\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(results)\n",
    "    \n",
    "    # Clean display with better styling\n",
    "    print(f\"\\nðŸ“Š PROMPT COMPARISON (Temperature: {temp})\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for i, row in df.iterrows():\n",
    "        print(f\"\\nðŸ”¹ Prompt: {row['Prompt']}\")\n",
    "        print(f\"   Perplexity: {row['Perplexity']} | Confidence: {row['Confidence']}\")\n",
    "        print(f\"   Generated: \\\"{row['Generated']}\\\"\")\n",
    "        print()\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c61e60",
   "metadata": {},
   "source": [
    "## Interactive Comparison Tools\n",
    "lets get more interactive!  You can change the prompt(s) in the cell below as well as in the interactive window.  \n",
    "Take note of how temperature impacts probability but interestingly, not always in ways you might expect (probabilistic vs determinstic systems)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46efb5ff",
   "metadata": {},
   "source": [
    "**Perplexity Interpretation:**\n",
    "- **1-2**: Very predictable (factual completions)\n",
    "- **2-10**: Somewhat predictable  \n",
    "- **10-100**: Moderately uncertain\n",
    "- **100+**: High uncertainty/ambiguity\n",
    "- **1000+**: Very confused/ambiguous prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9fd634d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5e88bd4bd614ccbbd526c250c7e55cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Text(value='What day is today?', description='Prompt:'), FloatSlider(value=0.1, descriptâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6a4bf036e9e48458e2ae7a38d851808",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Text(value='Tomatoes are....', description='Prompt 1:'), Text(value='What are tomatoes?'â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ipywidgets import interact, interactive, Dropdown, Text, FloatSlider, VBox, HBox\n",
    "from IPython.display import display\n",
    "\n",
    "# Interactive single prompt temperature comparison\n",
    "@interact(\n",
    "    prompt=Text(value=\"What day is today?\", description=\"Prompt:\"),\n",
    "    temp1=FloatSlider(value=0.1, min=0.0, max=2.0, step=0.1, description=\"Temp 1:\"),\n",
    "    temp2=FloatSlider(value=0.7, min=0.0, max=2.0, step=0.1, description=\"Temp 2:\"),\n",
    "    temp3=FloatSlider(value=1.5, min=0.0, max=2.0, step=0.1, description=\"Temp 3:\"),\n",
    "    max_tokens=Dropdown(options=[10, 20, 50], value=20, description=\"Max tokens:\")\n",
    ")\n",
    "def interactive_temp_comparison(prompt, temp1, temp2, temp3, max_tokens):\n",
    "    \"\"\"Interactive temperature comparison\"\"\"\n",
    "    temps = [temp1, temp2, temp3]\n",
    "    results = compare_temps(prompt, temps, max_tokens)\n",
    "    return results\n",
    "\n",
    "# Interactive prompt comparison\n",
    "@interact(\n",
    "    prompt1=Text(value=\"Tomatoes are....\", description=\"Prompt 1:\"),\n",
    "    prompt2=Text(value=\"What are tomatoes?\", description=\"Prompt 2:\"),\n",
    "    prompt3=Text(value=\"Tomatos!\", description=\"Prompt 3:\"),\n",
    "    temperature=FloatSlider(value=0.7, min=0.0, max=2.0, step=0.1, description=\"Temperature:\"),\n",
    "    max_tokens=Dropdown(options=[10, 20, 50], value=20, description=\"Max tokens:\")\n",
    ")\n",
    "def interactive_prompt_comparison(prompt1, prompt2, prompt3, temperature, max_tokens):\n",
    "    \"\"\"Interactive prompt comparison\"\"\"\n",
    "    prompts = [p for p in [prompt1, prompt2, prompt3] if p.strip()]\n",
    "    results = compare_prompts(prompts, temperature, max_tokens)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8ffde7-1181-4eda-96ee-d0723f25736e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
